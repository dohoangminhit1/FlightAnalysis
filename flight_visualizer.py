from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc, count
import json
import requests

# 1. Táº¡o Spark session
spark = SparkSession.builder \
    .appName("Flight Word Cloud to Elasticsearch") \
    .getOrCreate()

# 2. Äá»c dá»¯ liá»‡u tá»« HDFS (toÃ n bá»™ cÃ¡c file)
df = spark.read.csv("hdfs://hadoop-namenode:9000/input/*.csv", header=True, inferSchema=True)

# 3. Chá»n cá»™t thÃ nh phá»‘ Ä‘áº¿n vÃ  loáº¡i bá» giÃ¡ trá»‹ null
df_dest = df.select("DestCityName").na.drop()

# 4. Äáº¿m táº§n suáº¥t xuáº¥t hiá»‡n cá»§a tá»«ng thÃ nh phá»‘ Ä‘áº¿n
dest_count = df_dest.groupBy("DestCityName") \
    .agg(count("*").alias("count")) \
    .orderBy(desc("count"))

# 5. Gá»­i dá»¯ liá»‡u lÃªn Elasticsearch
es_url = "http://elasticsearch:9200/flight_wordcloud/_doc"
headers = {"Content-Type": "application/json"}

for row in dest_count.limit(100).collect():  # Giá»›i háº¡n 100 tá»« nhiá»u nháº¥t cho Ä‘áº¹p
    doc = {
        "city": row["DestCityName"],
        "count": int(row["count"])
    }
    res = requests.post(es_url, headers=headers, data=json.dumps(doc))
    if res.status_code in (200, 201):
        print(f"âœ… ÄÃ£ gá»­i {row['DestCityName']} ({row['count']}) vÃ o Elasticsearch.")
    else:
        print(f"âŒ Lá»—i gá»­i {row['DestCityName']}: {res.status_code} - {res.text}")

print("ğŸŒˆ Word cloud data Ä‘Ã£ sáºµn sÃ ng trong Elasticsearch!")
